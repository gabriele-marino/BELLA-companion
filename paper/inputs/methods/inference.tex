\subsection{Inference settings}

For each scenario, we generated 100 phylogenetic trees with tip counts ranging from 200 to 500, and performed inference on each of them using the BDMM-Prime package~\cite{Vaughan2025BDMM} implemented in BEAST2~\cite{Bouckaert2019BEAST2}. Each experiment was repeated with multiple inference approaches: a predictor-agnostic model, a GLM implementation~\cite{valenzuela2021comprehensive}, and three different Bayesian MLP architectures. The MLPs used in our analyses had hidden layer sizes of \( (3, 2) \), \( (16, 8) \), and \( (32, 16) \) neurons, with ReLU activations in the hidden layers and a sigmoid activation in the output layer. For analyses requiring inference of multiple parameters (e.g., speciation and extinction rates in FBD scenarios), a separate MLP was used for each target parameter, so that the output layer of each MLP consisted of a single neuron. The size of the input layer varied depending on the experiment and was set equal to the number of predictors characterizing that experiment. In accordance with best practices for GLMs and MLPs, we normalized each predictor to the range \( [0, 1] \) prior to inference.

All MCMC runs were executed with a chain length of 10,000,000 steps. For predictor-agnostic and GLM models, we used uniform priors: \( \mathcal{U}(1, 5) \) for reproduction numbers, \( \mathcal{U}(0, 0.05) \) for migration rates, and \( \mathcal{U}(0, 2) \) for speciation and extinction rates. For the MLPs, we instead applied a normal prior \( \mathcal{N}(0, 1) \) to the weights, while constraining the output via a sigmoid output activation function so that predictions remained within the same bounds as the uniform priors defined for predictor-agnostic and GLM models. For all analyses, we discarded the first 10\% of samples as burn-in and assessed convergence by verifying that the effective sample size (ESS) for each parameter exceeded 200.
